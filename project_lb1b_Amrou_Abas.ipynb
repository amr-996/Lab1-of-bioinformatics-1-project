{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project-lb1b - Amrou Abas.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD-24SJcnnGB"
      },
      "source": [
        "# 1- Selection of the initial dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rVG4wGo-epk"
      },
      "source": [
        "#Note:It was intended as much as possible clearing ambiguities in the following recapitulation of the codes used. \n",
        "#However the codes also depend on the directory in which the code is excuted\n",
        "#All the files mentioned are present in the supplimentary materials\n",
        "\n",
        "\n",
        "\n",
        "#Through rscb PDB, 167 sequences generated in the (rcsb_pdb_custom_report_20210501073759.csv) file\n",
        "#Printing only the wanted columns:\n",
        "awk -F \",\" '{print $(NF-1),$5,$3,$4}'  rcsb_pdb_custom_report_20210501073759.csv | tr -d '\"' |tail -n +2 >clean_pdb.txt\n",
        "\n",
        "#cleaning the file from the potentially co-crystallized sequences:\n",
        "awk '{if ($4>=40 && $4<=80) print $0}' clean_pdb.txt >clean_pdb_len_40_80.txt\n",
        "\n",
        "#Now, making another search to match the info from the first search with one to minimize any possible error probability\n",
        "\n",
        "# File (data123.csv) is the result of using 3TGI chain I against PDB:\n",
        "awk -F ',' '{print toupper($2),$3,$4}' data123.csv |tail -n +2 >3tgi_aln.list\n",
        "\n",
        "# Getting the columns that we need in the second list and removing the top row, and make the entry id/chain uppercase\n",
        "awk -F ',' '{print toupper($2),$3,$4}' data123.csv |tail -n +2 >3tgi_aln.list\n",
        "\n",
        "# getting the 2 lists to be arranged so that only one column in both:\n",
        "awk '{print $1$2}' clean_pdb_len_40_80.txt  |sort -u >list1\n",
        "\n",
        "awk '{print $1}' 3tgi_aln.list  |sort -u >list2\n",
        "\n",
        "#To create from clean_pdb_len_40_80.txt to create a file that generates an entry id and the chain concatenated and the sequence, generating a fasta file.\n",
        "awk '{print $1$2, $3}' clean_pdb_len_40_80.txt  >clean_pdb.seq\n",
        "\n",
        "#then using the python code (compare.py) to get the common sequences between the following 2 files:\n",
        "python compare.py clean_pdb.seq list2 |awk '{print \">\"$1;print$2}' >comm_seq.fasta \n",
        "\n",
        "\n",
        "#Now we are able to cluster them based on sequence similarity using blastclust or using cd-hit platform\n",
        "#In cd-hit, we used 90 percent as a cutoff, and a minimal alignment coverage for the shorter sequence of 80 percent.\n",
        "#Clustering them into 23 clusters, obtaining the sorted version and cleaning the file using the command:\n",
        "awk '{if (substr($0,1,1)==\">\") {print \"\" } else {printf \"%s \",substr($3,2,5)}}' cd-hit.cluster |tail -n +2 >cdhit-seq.list\n",
        "\n",
        "#it was also done with Blastclust, resulting in 25 clusters in the file \n",
        "# Going with the output of (cd-hit.cluster), doing a for loop for the following code to obtain 23 SEPARATE clusters:\n",
        "sed -n %ip cdhit-seq.list |awk '{split($0,a,\" \"); for (j=1;j<=length(a);j++) {print substr(a[j],1,4)}}' |sort -u >cluster$i\n",
        "\n",
        "#then another for loop to get the separate seed files:\n",
        "Python compare.py resolu.idx cluster$i |sort -nk3 |head -n 1 | awk '{print substr($1,1,4)}' >seed$i\n",
        "\n",
        "#then using the \"cat\" command to concatenate the 23 seed represntatives, resulting in the file (list_pdb.txt), \n",
        "#then sorting it and adding the chain, Note:(file seedcombination is a file where the wanted chain is added manually)\n",
        "\n",
        "awk '{print substr($1,1,4)}' ../seedcombination.txt |sort -u >list_pdb.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2W2jwmGoPX6"
      },
      "source": [
        "# 2- Structural Alignment Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR1yRSrkp6s5"
      },
      "source": [
        "#Now we got a list of identifiers, we want to build a multiple structural alignment based on them.\n",
        "#Using a bash code, we will get the sequences of the 23 identifiers\n",
        "#Creating a chain directory \n",
        "\n",
        "mkdir chains\n",
        "\n",
        "#then getting whole pdb file of each one of the 23:\n",
        "\n",
        "for i in `cat list_pdb.txt`\n",
        "> do\n",
        "> wget -q https://files.rcsb.org/view/$i.pdb\n",
        "\n",
        "> done\n",
        "\n",
        "#To space them out:\n",
        "\n",
        "awk '{print substr($1,1,4),substr($1,5,1)}' ../seedcombination.txt |sort -u >list_chain.txt\n",
        "\n",
        "#Using the (selch.sh) shell script to choose the chains:\n",
        "\n",
        "awk '{print \"./selch.sh\",$1\".pdb\",$2\" >chains/\"$1$2\".pdb\"}' list_chain.txt >run.sh\n",
        "\n",
        "#Then to generate a list of pdbfiles containing all the chains of the protein we are trying to align\n",
        "\n",
        "/bin/bash run.sh\n",
        "\n",
        "#Generating the zipped folder to analyze it using Mtmalign to do multiple structure alignment\n",
        "\n",
        "tar -czvf chains.tar.gz chains/\n",
        "\n",
        "#Removing a sequence of them to make the alignment more compact\n",
        "\n",
        "rm 1D0DA.pdb\n",
        "\n",
        "#Compress again, and then download the output\n",
        "\n",
        "wget https://yanglab.nankai.edu.cn/mTM-align/output/mTM016214/seq.fasta -O tm-ali.fasta\n",
        "\n",
        "#Cleaning the file\n",
        "\n",
        "awk '{if (substr($0,1,1)==\">\") {print \"\\n\"$0} else {printf \"%s\",$0}}' tm-ali.fasta  |tail -n +2 >tm-ali.clean\n",
        "\n",
        "#Using also pdbe to make another alignment (as an extra check) is available (fastapdbefoldMSA.seq).\n",
        "#Cleaning the file, removing extra rows, cutting the gapped edges:\n",
        "\n",
        "awk '{if (substr($0,1,1)==\">\") {printf \"\\n%s \",$0} else {printf \"%s\",$0}}' tm-ali.fasta |awk '{print substr($1,1,6); print substr($2,28,61)}' |tail -n +3 >bpti_kunitz.ali\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xss0fhXqy4c-"
      },
      "source": [
        "# 3- HMM Generation using HMMER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szcCAoZKzBG2"
      },
      "source": [
        "hmmbuild bpti_kunitz.hmm bpti_kunitz.ali"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2RRrGbcz5kV"
      },
      "source": [
        "# 4- Model Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQHspNNt0AUn"
      },
      "source": [
        "#Using the following codes in Uniprot to get the sets of positives and negatives respectively\n",
        "#Positives:\n",
        "\n",
        "database:(type:pfam pf00014) length:[40 TO *] AND reviewed:yes\n",
        "\n",
        "#Negatives:\n",
        "\n",
        "NOT database:(type:pfam pf00014) length:[40 TO *] AND reviewed:yes\n",
        "\n",
        "#Unzipping the 2 files\n",
        "\n",
        "zcat uniprot-database\\ \\(type\\ pfam+pf00014\\)+length\\ \\[40+TO+\\ \\]-filtered-rev--.list.gz  >positives.txt\n",
        "\n",
        "zcat uniprot-NOT+database\\ \\(type\\ pfam+pf00014\\)+length\\ \\[40+TO+\\ \\]+reviewed--.fasta.gz >negatives.txt\n",
        "\n",
        "#Getting just the list of idenitifiers\n",
        "\n",
        "cat positives.fasta |awk '{if (substr($0,1,1)==\">\") {split($0,a,\"|\"); print a[2]}}' |less >positives.id\n",
        "\n",
        "cat negatives.fasta |awk '{if (substr($0,1,1)==\">\") {split($0,a,\"|\"); print a[2]}}' |less >negatives.id\n",
        "\n",
        "\n",
        "#Now, we need to sort them and then to select the first half and the other half\n",
        "# 1- Sorting them randomly:\n",
        "sort -R negatives.id >negatives.rids\n",
        "sort -R positives.id >positives.rids\n",
        "\n",
        "\n",
        "#2- Cut the halves to four halves:\n",
        "#  i-positives:\n",
        "head -n 178 positives.rids >set_pos1.ids\n",
        "tail -n +179 pos\titives.rids >set_pos2.ids\n",
        "\n",
        "#   ii-negatives:\n",
        "head -n 277521 negatives.rids >set_neg1.ids\n",
        "tail -n +277522 negatives.rids >set_neg2.ids\n",
        "\n",
        "#Now we need to generate the fasta file that are needed for running the program, \n",
        "\n",
        "#we should write a very basic script that takes an input a list of identifiers and a fasta file and extracts fasta file from the file that you provided as an input.\n",
        "\n",
        "python3 select-seqs.py pdb/set_pos1.ids positives.fasta >set_pos1.fasta\n",
        "python3 select-seqs.py pdb/set_pos2.ids positives.fasta >set_pos2.fasta\n",
        "\n",
        "python3 select-seqs.py pdb/set_neg1.ids negatives.fasta >set_neg1.fasta\n",
        "python3 select-seqs.py pdb/set_neg2.ids negatives.fasta >set_neg2.fasta\n",
        "\n",
        "#We also will disactivate the choice to discard the sequences that \n",
        "#do not have a very high seed match, and to get a better calculation for the match between the model and the sequence: --max\n",
        "#-Z <x>        : set # of comparisons done, for E-value calculation\n",
        "\n",
        "hmmsearch -Z 1 --noali --max --tblout set_neg1.out bpti_kunitz.hmm set_neg1.fasta\n",
        "\n",
        "hmmsearch -Z 1 --noali --max --tblout set_neg2.out bpti_kunitz.hmm set_neg2.fasta\n",
        "\n",
        "hmmsearch -Z 1 --noali --max --tblout set_pos1.out bpti_kunitz.hmm set_pos1.fasta\n",
        "\n",
        "hmmsearch -Z 1 --noali --max --tblout set_pos2.out bpti_kunitz.hmm set_pos2.fasta\n",
        "\n",
        "#Not all queries will produce hits, so calculating the hits:\n",
        "\n",
        "grep -v \"^#\" set_pos1.out |awk '{print $1,$8,1}' >set_pos1.res\n",
        "grep -v \"^#\" set_pos2.out |awk '{print $1,$8,1}' >set_pos2.res\n",
        "\n",
        "grep -v \"^#\" set_neg1.out |awk '{print $1,$8,0}' >set_neg1.res\n",
        "grep -v \"^#\" set_neg2.out |awk '{print $1,$8,0}' >set_neg2.res\n",
        "\n",
        "#The process of recovering the missing sequences from the negatives\n",
        "\n",
        "comm -23 <( sort set_neg1.ids) <(awk '{print $1}' set_neg1.res |sort) | awk '{print $1,10,0}'   >set_neg1.add\n",
        "comm -23 <( sort set_neg2.ids) <(awk '{print $1}' set_neg2.res |sort) | awk '{print $1,10,0}'   >set_neg2.add\n",
        "\n",
        "#Setting the complete set1 and set2\n",
        "\n",
        "cat set_neg1.res set_neg1.add set_pos1.res >set_all1.res\n",
        "cat set_neg2.res set_neg2.add set_pos2.res >set_all2.res\n",
        "\n",
        "#Getting the threshold of set#1 using a python code that calculates the performance (in the zipped file)\n",
        "python performance.py set_all1.res |sort -nrk 6\n",
        "\n",
        "#Applying optimal threshold of set1 to set2\n",
        "python performance.py set_all2.res 1e-05 \n",
        "\n",
        "#Making a total set\n",
        "awk '{p=0; if ($2<1e-5) {p=1}; print $1,p,$3}' set_all2.res > set_all.res\n",
        "awk '{p=0; if ($2<1e-11) {p=1}; print $1,p,$3}' set_all1.res >> set_all.res\n",
        "\n",
        "#getting the total ACC and MCC using a modified performance code \n",
        "\n",
        "python performance_all.py set_all.res 0.5\n",
        "\n",
        "#Calculating false positives:\n",
        "awk '{if ($2==1 && $3==0) print $1\":false positives\"}' set_all.res\n",
        "\n",
        "#Calculating false negatives:\n",
        "awk '{if ($2==0 && $3==1) print $1\":false negative\"}' set_all.res\n",
        "\n",
        "#running the model against the misclassified sequences\n",
        "\n",
        "#1- false negatives:\n",
        "hmmsearch bpti_kunitz.hmm D3GGZ8.fasta |less\n",
        "hmmsearch bpti_kunitz.hmm P86963.fasta |less\n",
        "hmmsearch bpti_kunitz.hmm Q11101.fasta |less\n",
        "hmmsearch bpti_kunitz.hmm O62247.fasta |less\n",
        "\n",
        "#2- false postives:\n",
        "hmmsearch bpti_kunitz.hmm P56409.fasta |less\n",
        "hmmsearch bpti_kunitz.hmm P40500.fasta |less\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}